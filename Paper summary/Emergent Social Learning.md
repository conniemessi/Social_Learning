Emergent Social Learning via Multi-agent Reinforcement Learning


Summary:
We are able to obtain generalized social learning strategies that enable agents to: i) discover complex skills that cannot be learned from single-agent training; ii) adapt to new environments online by taking cues from experts present in the new environment.

æ€»æ‹¬ï¼š
æˆ‘ä»¬èƒ½å¤Ÿè·å¾—å¹¿ä¹‰ç¤¾ä¼šå­¦ä¹ ç­–ç•¥ï¼Œä½¿æ™ºèƒ½ä½“èƒ½å¤Ÿï¼šiï¼‰å‘ç°ä»å•ä¸€æ™ºèƒ½ä½“è®­ç»ƒä¸­æ— æ³•å­¦åˆ°çš„å¤æ‚æŠ€èƒ½ï¼›iiï¼‰é€šè¿‡ä»æ–°ç¯å¢ƒä¸­å­˜åœ¨çš„ä¸“å®¶é‚£é‡Œè·å¾—çº¿ç´¢ï¼Œåœ¨çº¿é€‚åº”æ–°ç¯å¢ƒã€‚


The difference from imitation learning:
Interactions in social learning are directly mediated by the environment. A novice can acquire problem-solving skills by watching a selfish expert perform the same task without explicitly imitating the expert. Beyond simple imitation, social learning enables individuals to develop complex behaviors or innovations from others that a person would not be able to discover from scratch in a lifetime.

ä¸æ¨¡ä»¿å­¦ä¹ çš„åŒºåˆ«ï¼š
ç¤¾ä¼šå­¦ä¹ ä¸­çš„äº¤äº’æ˜¯ç›´æ¥ç”±ç¯å¢ƒä»‹å¯¼çš„ã€‚ä¸€ä¸ªæ–°æ‰‹é€šè¿‡è§‚å¯Ÿä¸€ä¸ªè‡ªç§çš„ä¸“å®¶å®Œæˆç›¸åŒçš„ä»»åŠ¡å°±å¯ä»¥è·å¾—è§£å†³é—®é¢˜çš„èƒ½åŠ›ï¼Œè€Œæ— éœ€æ˜ç¡®æ¨¡ä»¿ä¸“å®¶ã€‚é™¤äº†ç®€å•çš„æ¨¡ä»¿ï¼Œç¤¾ä¼šå­¦ä¹ ä½¿å¾—ä¸ªäººèƒ½å¤Ÿä»ä»–äººçš„åŸºç¡€ä¸Šå‘å±•å‡ºå¤æ‚çš„è¡Œä¸ºæˆ–åˆ›æ–°ï¼Œè¿™æ˜¯ä¸€ä¸ªäººåœ¨ä¸€ç”Ÿä¸­æ— æ³•ä»é›¶å¼€å§‹å‘ç°çš„ã€‚

å…·å¤‡ç¤¾ä¼šå­¦ä¹ èƒ½åŠ›çš„äººç±»å’ŒåŠ¨ç‰©èƒ½å¤Ÿåœ¨æ²¡æœ‰å¤–éƒ¨ç³»ç»Ÿä»‹å…¥çš„æƒ…å†µä¸‹ï¼Œè¯†åˆ«ä¸è‡ªèº«å…´è¶£ç›¸å…³çš„
ä»»åŠ¡ä¸“å®¶å¹¶ä»ä»–ä»¬é‚£é‡Œå­¦ä¹ ã€‚åœ¨æ–°ç¯å¢ƒä¸­ï¼Œç¤¾ä¼šå­¦ä¹ ä½¿å¾—èƒ½å¤Ÿ"åœ¨çº¿é€‚åº”"æ–°ä»»åŠ¡ï¼Œè¿™æ˜¯æ¨¡ä»¿å­¦ä¹ æ— æ³•åšåˆ°çš„ã€‚

Setting:
Humans and animals with social learning capabilities are able to identify and learn from experts on tasks relevant to their interests without the intervention of external systems. In new environments, social learning enables "online adaptation" to new tasks, which imitation learning cannot do.


Focus on two hypotheses related to social learning: H1) Social learning can discover more complex strategies that are difficult to discover through expensive individual exploration; H2) Social learning can be used when the social learning strategy encodes how to learn from cues from experts , online adaptation in new environments.

Settings representative of real-world human environments: Multi-agent reinforcement learning (MARL). This setting represents real-world multi-agent problems, such as autonomous driving.

When social learning occurs, it enables agents to learn more complex behaviors than agents and experts trained alone can achieve on their own.

é—®é¢˜è®¾ç½®
å…³æ³¨ä¸ç¤¾ä¼šå­¦ä¹ ç›¸å…³çš„ä¸¤ä¸ªå‡è®¾ï¼šH1ï¼‰ç¤¾ä¼šå­¦ä¹ å¯ä»¥å‘ç°é€šè¿‡æ˜‚è´µçš„ä¸ªä½“æ¢ç´¢éš¾ä»¥å‘ç°çš„æ›´å¤æ‚çš„ç­–ç•¥ï¼›H2ï¼‰ç¤¾ä¼šå­¦ä¹ å¯ä»¥ç”¨äºå½“ç¤¾ä¼šå­¦ä¹ ç­–ç•¥ç¼–ç äº†å¦‚ä½•ä»ä¸“å®¶çš„çº¿ç´¢ä¸­å­¦ä¹ æ—¶ï¼Œåœ¨æ–°ç¯å¢ƒä¸­è¿›è¡Œåœ¨çº¿é€‚åº”ã€‚

ä»£è¡¨çœŸå®ä¸–ç•Œäººç±»ç¯å¢ƒçš„è®¾ç½®ï¼šå¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ï¼ˆMARLï¼‰ã€‚è¿™ä¸ªè®¾ç½®ä»£è¡¨äº†ç°å®ä¸–ç•Œçš„å¤šæ™ºèƒ½ä½“é—®é¢˜ï¼Œæ¯”å¦‚è‡ªåŠ¨é©¾é©¶ã€‚

å½“ç¤¾ä¼šå­¦ä¹ å‘ç”Ÿæ—¶ï¼Œå®ƒä½¿ä»£ç†èƒ½å¤Ÿå­¦ä¹ åˆ°æ¯”å•ç‹¬è®­ç»ƒçš„ä»£ç†å’Œä¸“å®¶è‡ªå·±è¿˜è¦å¤æ‚çš„è¡Œä¸ºã€‚

Model-Free Sparse Reward Social Learning
Consider an example environment where an agent must first pick up a key and use it to get through a door to a target. In this sparse reward, hard-to-explore environment, the agent only receives rewards when it reaches the goal; at other times, the reward is 0. Suppose there is an unlimited supply of keys, and when one agent passes through the door, it closes - so that other agents cannot change the environment to make the task easier. Instead, the expert agent can exhibit a new state s~ that is difficult to generate through random exploration: open the door. We hope that the novice agent will learn this exemplary state s~ by updating its internal representation, and eventually learn how to generate this state.

Consider Q-learning, where Q(a,s)=ğ”¼Ï€{[}âˆ‘t=0âˆÎ³t rt+1|a,s{]} represents the total expected reward of taking action a in state s. Since the agent will only receive 0 rewards during training, all Q values will approach zero. Even if the agent observes a useful new state s~, since Q(a,s)â†’0,âˆ€aâˆˆğ’œ,sâˆˆğ’®, the goal of Q-learning becomes:
Q(s~,a)=r+Î³maxaâ€²Q(sâ€²,aâ€²)=0+Î³0=0(2)
Therefore, this goal forces the value of Q(s~,a) to be zero. In this case, in order to produce an output of zero, there is no need to model entering or leaving state s~, since all other Q values are already zero. As can be seen from both cases above, before a model-free reinforcement learning agent receives rewards from the environment, it will have a hard time modeling new states or transition dynamics, making learning from expert cues particularly difficult.

Social Learning with Auxiliary Losses
A method for increasing model-based prediction losses in novice agents. Specifically, we add an additional layer Î¸A based on the policy network encoding the current state st, as shown in Figure 1. We introduce an unsupervised mean absolute error (MAE) auxiliary loss to train the network to predict the next state st+1, given the current state st and action at:

If the new presentation state is in a trajectory, s^kâˆˆ(0,T), then the term |s~kâˆ’s^k| will be part of the goal. It is non-zero only if the agent learns to perfectly predict the new presentation state. Thus, cues from the expert will provide gradients that enable a novice to improve its representation of the world, even if it does not receive any reward from the demonstration. This architecture also implicitly improves the agent's ability to model the strategies of other agents, since in order to accurately predict the next state, it must correctly predict the actions of other agents. It can do this without explicit access to the other agent's state or actions, or even any labels indicating which elements in the environment constitute the other agent. We call our approach social learning with an auxiliary prediction loss and hypothesize that it will improve the agent's ability to learn from expert cues.

Social Learning Environments
Humans and animals are most likely to rely on social learning when individual learning is difficult or dangerous
A novel environment specifically designed to encourage social learning by making individual exploration difficult and expensive and introducing prestige cues is introduced.



Model-Free Sparse Reward Social Learning
è€ƒè™‘ä¸€ä¸ªç¤ºä¾‹ç¯å¢ƒï¼Œå…¶ä¸­ä»£ç†å¿…é¡»å…ˆæ‹¿èµ·ä¸€æŠŠé’¥åŒ™ï¼Œå¹¶ä½¿ç”¨å®ƒé€šè¿‡é—¨åˆ°è¾¾ç›®æ ‡ã€‚åœ¨è¿™ä¸ªç¨€ç–å¥–åŠ±ã€éš¾ä»¥æ¢ç´¢çš„ç¯å¢ƒä¸­ï¼Œä»£ç†åªæœ‰åœ¨åˆ°è¾¾ç›®æ ‡æ—¶æ‰èƒ½æ¥æ”¶åˆ°å¥–åŠ±ï¼›å…¶ä»–æ—¶é—´ï¼Œå¥–åŠ±éƒ½ä¸º0ã€‚å‡è®¾æœ‰æ— é™ä¾›åº”çš„é’¥åŒ™ï¼Œå¹¶ä¸”å½“ä¸€ä¸ªä»£ç†é€šè¿‡é—¨åï¼Œå®ƒä¼šå…³é—­ - è¿™æ ·å…¶ä»–ä»£ç†å°±æ— æ³•æ”¹å˜ç¯å¢ƒä»¥ä½¿ä»»åŠ¡å˜å¾—æ›´å®¹æ˜“ã€‚ç›¸åï¼Œä¸“å®¶ä»£ç†å¯ä»¥å±•ç¤ºä¸€ä¸ªé€šè¿‡éšæœºæ¢ç´¢å¾ˆéš¾äº§ç”Ÿçš„æ–°çŠ¶æ€s~ï¼šæ‰“å¼€é—¨ã€‚æˆ‘ä»¬å¸Œæœ›æ–°æ‰‹ä»£ç†èƒ½å¤Ÿé€šè¿‡æ›´æ–°å…¶å†…éƒ¨è¡¨ç¤ºæ¥å­¦ä¹ è¿™ä¸ªç¤ºèŒƒçŠ¶æ€s~ï¼Œå¹¶æœ€ç»ˆå­¦ä¼šå¦‚ä½•äº§ç”Ÿè¿™ä¸ªçŠ¶æ€ã€‚

è€ƒè™‘Qå­¦ä¹ ï¼Œå…¶ä¸­Q(a,s)=ğ”¼Ï€{[}âˆ‘t=0âˆÎ³t rt+1âˆ£a,s{]}è¡¨ç¤ºåœ¨çŠ¶æ€sä¸­é‡‡å–åŠ¨ä½œaçš„æ€»æœŸæœ›å¥–åŠ±ã€‚ç”±äºè®­ç»ƒè¿‡ç¨‹ä¸­ä»£ç†åªä¼šæ”¶åˆ°0çš„å¥–åŠ±ï¼Œæ‰€æœ‰çš„Qå€¼éƒ½ä¼šè¶‹è¿‘äºé›¶ã€‚å³ä½¿ä»£ç†è§‚å¯Ÿåˆ°ä¸€ä¸ªæœ‰ç”¨çš„æ–°çŠ¶æ€s~ï¼Œç”±äºQ(a,s)â†’0,âˆ€aâˆˆğ’œ,sâˆˆğ’®ï¼ŒQå­¦ä¹ çš„ç›®æ ‡å˜ä¸ºï¼š
Q(s~,a)=r+Î³maxaâ€²Q(sâ€²,aâ€²)=0+Î³0=0(2)
å› æ­¤ï¼Œè¯¥ç›®æ ‡è¿«ä½¿Q(s~,a)çš„å€¼ä¸ºé›¶ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä¸ºäº†äº§ç”Ÿä¸€ä¸ªé›¶çš„è¾“å‡ºï¼Œä¸éœ€è¦å¯¹è¿›å…¥æˆ–ç¦»å¼€çŠ¶æ€s~è¿›è¡Œå»ºæ¨¡ï¼Œå› ä¸ºæ‰€æœ‰å…¶ä»–çš„Qå€¼å·²ç»ä¸ºé›¶ã€‚ä»ä¸Šè¿°ä¸¤ç§æƒ…å†µå¯ä»¥çœ‹å‡ºï¼Œåœ¨æ— æ¨¡å‹å¼ºåŒ–å­¦ä¹ ä»£ç†ä»ç¯å¢ƒä¸­è·å¾—å¥–åŠ±ä¹‹å‰ï¼Œå®ƒå°†å¾ˆéš¾å¯¹æ–°çš„çŠ¶æ€æˆ–è½¬æ¢åŠ¨æ€è¿›è¡Œå»ºæ¨¡ï¼Œè¿™ä½¿å¾—ä»ä¸“å®¶çš„çº¿ç´¢ä¸­å­¦ä¹ å˜å¾—ç‰¹åˆ«å›°éš¾ã€‚

Social Learning with Auxiliary Losses
åœ¨æ–°æ‰‹ä»£ç†ä¸­å¢åŠ åŸºäºæ¨¡å‹çš„é¢„æµ‹æŸå¤±çš„æ–¹æ³•ã€‚å…·ä½“è€Œè¨€ï¼Œæˆ‘ä»¬åœ¨ç­–ç•¥ç½‘ç»œå¯¹å½“å‰çŠ¶æ€ st è¿›è¡Œç¼–ç çš„åŸºç¡€ä¸Šï¼Œæ·»åŠ äº†é¢å¤–çš„å±‚ Î¸Aï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚æˆ‘ä»¬å¼•å…¥äº†ä¸€ä¸ªæ— ç›‘ç£çš„å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰è¾…åŠ©æŸå¤±ï¼Œç”¨äºè®­ç»ƒç½‘ç»œæ¥é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€ st+1ï¼Œç»™å®šå½“å‰çŠ¶æ€ st å’ŒåŠ¨ä½œ atï¼š

å¦‚æœæ–°çš„æ¼”ç¤ºçŠ¶æ€åœ¨ä¸€ä¸ªè½¨è¿¹ä¸­ï¼Œs^kâˆˆ(0,T)ï¼Œé‚£ä¹ˆæœ¯è¯­ |s~kâˆ’s^k| å°†æˆä¸ºç›®æ ‡çš„ä¸€éƒ¨åˆ†ã€‚åªæœ‰å½“ä»£ç†å­¦ä¼šå®Œç¾åœ°é¢„æµ‹æ–°çš„æ¼”ç¤ºçŠ¶æ€æ—¶ï¼Œå®ƒæ‰ä¸ä¸º0ã€‚å› æ­¤ï¼Œæ¥è‡ªä¸“å®¶çš„çº¿ç´¢å°†æä¾›æ¢¯åº¦ï¼Œä½¿æ–°æ‰‹èƒ½å¤Ÿæ”¹å–„å…¶å¯¹ä¸–ç•Œçš„è¡¨ç¤ºï¼Œå³ä½¿å®ƒæ²¡æœ‰ä»æ¼”ç¤ºä¸­è·å¾—ä»»ä½•å¥–åŠ±ã€‚è¿™ç§æ¶æ„è¿˜éšå«åœ°æé«˜äº†ä»£ç†å»ºæ¨¡å…¶ä»–ä»£ç†ç­–ç•¥çš„èƒ½åŠ›ï¼Œå› ä¸ºä¸ºäº†å‡†ç¡®é¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€ï¼Œå®ƒå¿…é¡»æ­£ç¡®é¢„æµ‹å…¶ä»–ä»£ç†çš„åŠ¨ä½œã€‚å®ƒå¯ä»¥åœ¨æ²¡æœ‰æ˜ç¡®è®¿é—®å…¶ä»–ä»£ç†çš„çŠ¶æ€æˆ–åŠ¨ä½œçš„æƒ…å†µä¸‹åšåˆ°è¿™ä¸€ç‚¹ï¼Œç”šè‡³æ²¡æœ‰ä»»ä½•æŒ‡ç¤ºç¯å¢ƒä¸­å“ªäº›å…ƒç´ æ„æˆå…¶ä»–ä»£ç†çš„æ ‡ç­¾ã€‚æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ–¹æ³•ç§°ä¸ºå¸¦æœ‰è¾…åŠ©é¢„æµ‹æŸå¤±çš„ç¤¾äº¤å­¦ä¹ ï¼Œå¹¶å‡è®¾å®ƒå°†æé«˜ä»£ç†ä»ä¸“å®¶çº¿ç´¢ä¸­å­¦ä¹ çš„èƒ½åŠ›ã€‚

Social Learning Environments
å½“ä¸ªä½“å­¦ä¹ å›°éš¾æˆ–å­˜åœ¨å±é™©æ—¶ï¼Œäººç±»å’ŒåŠ¨ç‰©æœ€æœ‰å¯èƒ½ä¾èµ–ç¤¾ä¼šå­¦ä¹ 
å¼•å…¥äº†ä¸€ä¸ªä¸“é—¨è®¾è®¡çš„æ–°é¢–ç¯å¢ƒï¼Œé€šè¿‡ä½¿ä¸ªä½“æ¢ç´¢å›°éš¾å’Œæ˜‚è´µï¼Œå¹¶å¼•å…¥å¨æœ›æç¤ºï¼Œæ¥é¼“åŠ±ç¤¾ä¼šå­¦ä¹ ã€‚
